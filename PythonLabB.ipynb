{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold or i < 10:\n",
    "            i = i + 1\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue\n",
    "                    \n",
    "                tmpV = 0\n",
    "                currval = [0,0,0,0]\n",
    "            \n",
    "                \n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    tmpQN = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                        tmpQN = tmpQN + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * Vnew[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                    currval[action_idx] = tmpQN;\n",
    "                \n",
    "                policy[state_idx, :] = 0\n",
    "                policy[state_idx, currval.index(max(currval))] = 1\n",
    "                \n",
    "                Vnew[state_idx] = tmpV\n",
    "                \n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, policy           \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "#         plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action[0]]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of that policy is : [6.39769088e-04 2.56653830e-03 1.00525119e-02 3.66139477e-02\n",
      " 1.31539033e-01 1.62613638e-04 3.21203067e-02 1.31539033e-01\n",
      " 5.27060198e-01 3.99720629e-05 8.31250483e-03 2.11375926e+00\n",
      " 4.77210983e-05 1.28683126e-03 3.25573254e-02 1.01745743e+00\n",
      " 8.27889044e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy = np.zeros((grid.state_size, grid.action_size))\n",
    "\n",
    "threshold = 0.00001\n",
    "discount = 0.3\n",
    "val, Policy = grid.value_iteration(Policy, threshold, discount)\n",
    "\n",
    "\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "    \n",
    "grid.draw_deterministic_policy(list(map(lambda x: np.where(x==1)[0], Policy)))\n",
    "\n",
    "\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
